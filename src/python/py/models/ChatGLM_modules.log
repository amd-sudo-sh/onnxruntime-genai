Valid precision + execution provider combinations are: FP32 CPU, FP32 CUDA, FP16 CUDA, FP16 DML, INT4 CPU, INT4 CUDA, INT4 DML
Extra options: {}
GroupQueryAttention (GQA) is used in this model.
##########
ChatGLMForConditionalGeneration(
  (transformer): ChatGLMModel(
    (embedding): Embedding(
      (word_embeddings): Embedding(65024, 4096)
    )
    (rotary_pos_emb): RotaryEmbedding()
    (encoder): GLMTransformer(
      (layers): ModuleList(
        (0-27): 28 x GLMBlock(
          (input_layernorm): RMSNorm()
          (self_attention): SelfAttention(
            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)
            (core_attention): CoreAttention(
              (attention_dropout): Dropout(p=0.0, inplace=False)
            )
            (dense): Linear(in_features=4096, out_features=4096, bias=False)
          )
          (post_attention_layernorm): RMSNorm()
          (mlp): MLP(
            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)
            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)
          )
        )
      )
      (final_layernorm): RMSNorm()
    )
    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)
  )
)
##########
ChatGLMModel(
  (embedding): Embedding(
    (word_embeddings): Embedding(65024, 4096)
  )
  (rotary_pos_emb): RotaryEmbedding()
  (encoder): GLMTransformer(
    (layers): ModuleList(
      (0-27): 28 x GLMBlock(
        (input_layernorm): RMSNorm()
        (self_attention): SelfAttention(
          (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)
          (core_attention): CoreAttention(
            (attention_dropout): Dropout(p=0.0, inplace=False)
          )
          (dense): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (post_attention_layernorm): RMSNorm()
        (mlp): MLP(
          (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)
          (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)
        )
      )
    )
    (final_layernorm): RMSNorm()
  )
  (output_layer): Linear(in_features=4096, out_features=65024, bias=False)
)
##########
Embedding(
  (word_embeddings): Embedding(65024, 4096)
)
##########
Embedding(65024, 4096)
Reading embedding layer
##########
RotaryEmbedding()
##########
GLMTransformer(
  (layers): ModuleList(
    (0-27): 28 x GLMBlock(
      (input_layernorm): RMSNorm()
      (self_attention): SelfAttention(
        (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)
        (core_attention): CoreAttention(
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (dense): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (post_attention_layernorm): RMSNorm()
      (mlp): MLP(
        (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)
        (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)
      )
    )
  )
  (final_layernorm): RMSNorm()
)
##########
ModuleList(
  (0-27): 28 x GLMBlock(
    (input_layernorm): RMSNorm()
    (self_attention): SelfAttention(
      (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)
      (core_attention): CoreAttention(
        (attention_dropout): Dropout(p=0.0, inplace=False)
      )
      (dense): Linear(in_features=4096, out_features=4096, bias=False)
    )
    (post_attention_layernorm): RMSNorm()
    (mlp): MLP(
      (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)
      (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)
    )
  )
)
##########
GLMBlock(
  (input_layernorm): RMSNorm()
  (self_attention): SelfAttention(
    (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)
    (core_attention): CoreAttention(
      (attention_dropout): Dropout(p=0.0, inplace=False)
    )
    (dense): Linear(in_features=4096, out_features=4096, bias=False)
  )
  (post_attention_layernorm): RMSNorm()
  (mlp): MLP(
    (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)
    (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)
  )
)
Reading decoder layer 0
['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_allocate_memory', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_is_hf_initialized', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'core_attention', 'cpu', 'cuda', 'dense', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'hidden_size_per_attention_head', 'ipu', 'layer_number', 'load_state_dict', 'modules', 'mtia', 'multi_query_attention', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_attention_heads_per_partition', 'num_multi_query_groups_per_partition', 'parameters', 'projection_size', 'qkv_hidden_size', 'query_key_value', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']
True
